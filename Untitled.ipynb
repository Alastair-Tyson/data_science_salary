{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:11:11.780490Z",
     "start_time": "2020-01-15T12:11:11.765587Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_result(result):\n",
    "    salary=[]\n",
    "    title=[]\n",
    "    company=[]\n",
    "    location=[]\n",
    "    for i in result.find_all('div',attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            salary.append(i.find('span',attrs={'class':'salaryText'}).text.strip())\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            title.append(i.find('div',attrs={'class':'title'}).text.strip())\n",
    "        except:\n",
    "            title.append(np.nan)\n",
    "        try:\n",
    "            company.append(i.find('span',attrs={'class':'company'}).text.strip())\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "        try:\n",
    "            location.append(i.find('span',attrs={'class':'location accessible-contrast-color-location'}).text.strip())\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "    df=pd.DataFrame({'Title':title,\n",
    "                     'Location':location,\n",
    "                     'Company':company,\n",
    "                     'Salary': salary})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:03:21.823328Z",
     "start_time": "2020-01-15T12:03:20.485692Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import timeÂ§\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:03:50.124413Z",
     "start_time": "2020-01-15T12:03:50.117285Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_salary(result):\n",
    "    salary=[]\n",
    "    for i in result.find_all('div',attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            salary.append(i.find('span',attrs={'class':'salaryText'}).text.strip())\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:03:50.418998Z",
     "start_time": "2020-01-15T12:03:50.411965Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_title(result):\n",
    "    title=[]\n",
    "    for i in result.find_all('div',attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            title.append(i.find('div',attrs={'class':'title'}).text.strip())\n",
    "        except:\n",
    "            title.append(np.nan)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:03:50.737493Z",
     "start_time": "2020-01-15T12:03:50.719633Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_company(result):\n",
    "    company=[]\n",
    "    for i in result.find_all('div',attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            company.append(i.find('span',attrs={'class':'company'}).text.strip())\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:03:51.057101Z",
     "start_time": "2020-01-15T12:03:51.050768Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_location(result):\n",
    "    location=[]\n",
    "    for i in result.find_all('div',attrs={'class':'jobsearch-SerpJobCard'}):\n",
    "        try:\n",
    "            location.append(i.find('span',attrs={'class':'location accessible-contrast-color-location'}).text.strip())\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:27:02.029271Z",
     "start_time": "2020-01-15T12:27:02.022409Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:34:53.314992Z",
     "start_time": "2020-01-15T12:34:35.173647Z"
    }
   },
   "outputs": [],
   "source": [
    "max_city={}\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'London']):\n",
    "    URL=\"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}\".format(city)\n",
    "    r = requests.get(URL)\n",
    "        \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    count=[i.text.strip() for i in soup.find_all('div',attrs={'id':'searchCountPages'})]\n",
    "    pattern = re.compile(r'[\\S]?[\\d]+')\n",
    "    page=re.findall(pattern, count[0])\n",
    "    max_city[city]=page[1]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:34:59.648264Z",
     "start_time": "2020-01-15T12:34:59.639733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Denver': '144',\n",
       " 'San+Francisco': '830',\n",
       " 'Philadelphia': '195',\n",
       " 'Atlanta': '222',\n",
       " 'Portland': '46',\n",
       " 'New+York': '869',\n",
       " 'Phoenix': '86',\n",
       " 'Seattle': '962',\n",
       " 'Chicago': '313',\n",
       " 'London': '32',\n",
       " 'Miami': '31',\n",
       " 'Los+Angeles': '287',\n",
       " 'Dallas': '241',\n",
       " 'Pittsburgh': '105',\n",
       " 'Austin': '161',\n",
       " 'Houston': '90'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:33:06.283858Z",
     "start_time": "2020-01-15T12:33:06.273500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'869'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'[\\S]?[\\d]+')\n",
    "page=re.findall(pattern, count[0])\n",
    "page[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-15T12:04:59.677851Z",
     "start_time": "2020-01-15T12:04:18.587239Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8cb939ee59a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Append to the full set of results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 1 # Set this to a high-value (5000) to generate more results. \n",
    "# Crawling more results, will also take much longer. First test your code on a small number of results and then expand.\n",
    "\n",
    "company=[]\n",
    "salary=[]\n",
    "location=[]\n",
    "title=[]\n",
    "\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'London']):\n",
    "    for start in range(0, max_city[city], 10):\n",
    "        URL=\"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\".format(city,start)\n",
    "        r = requests.get(URL)\n",
    "        \n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        c=extract_company(soup)\n",
    "        for comp in c:\n",
    "            company.append(comp)\n",
    "        s=extract_salary(soup)\n",
    "        for sal in s:\n",
    "            salary.append(sal)\n",
    "        l=extract_location(soup)\n",
    "        for loc in l:\n",
    "            location.append(loc)\n",
    "        t=extract_title(soup)\n",
    "        for tit in t:\n",
    "            title.append(tit)\n",
    "        # Append to the full set of results\n",
    "\n",
    "\n",
    "job_search=pd.DataFrame({'Title':title,\n",
    "                     'Location':location,\n",
    "                     'Company':company,\n",
    "                     'Salary': salary})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
